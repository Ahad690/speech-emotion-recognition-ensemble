# Speech Emotion Recognition using Ensemble Deep Learning

## Project Overview
**Course**: Artificial Neural Networks & Deep Learning (CS380)  
**Semester**: 5th Semester, BSAI  
**Author**: Ahad Imran  
**Accuracy**: 66.74% on test set  
**Architecture**: CNN + LSTM + Transformer Ensemble  

## Key Results
- **Dataset**: 11,682 samples (RAVDESS, TESS, CREMA-D)
- **Classes**: 7 emotions (neutral, happy, sad, angry, fear, disgust, surprise)
- **Best Performance**: Surprise (88.8%), Angry (76.8%)
- **Training Time**: 19s/epoch with dual GPU optimization

## Quick Start
```bash
# Clone repository
git clone https://github.com/Ahad690/speech-emotion-recognition-ensemble.git

# Install dependencies
pip install -r requirements.txt

# Run on Kaggle (recommended for GPU)
# Upload notebook to Kaggle and select GPU T4 x2
```

## ğŸš€ Quick Inference (Google Colab)

Run emotion recognition on any audio file:

```python
# ============================================
# Speech Emotion Recognition - Complete Setup & Inference
# ============================================
import os

# 1. Clone Repository
REPO_NAME = 'speech-emotion-recognition-ensemble'
GITHUB_USER = 'Ahad690'

if not os.path.exists(REPO_NAME):
    print("ğŸ“¥ Cloning repository...")
    !git clone https://github.com/{GITHUB_USER}/{REPO_NAME}.git
else:
    print("ğŸ“ Repository already exists, updating...")
    %cd {REPO_NAME}
    !git pull
    %cd ..
    
%cd {REPO_NAME}

# 2. Install Dependencies (including soundfile for torchaudio fallback)
print("\nğŸ“¦ Installing dependencies...")
!pip install -q torch torchaudio librosa matplotlib soundfile numpy scipy

# 3. Run Inference (randomly selects a .wav file from sample_audio/)
print("\nğŸ”® Running emotion recognition on random sample...")
!python inference.py

# 4. Play Audio Manually (plays the SAME file that was analyzed)
print("\nğŸ”Š Playing audio manually...")
from IPython.display import Audio, display

# Read the audio file that was just analyzed
if os.path.exists('.last_audio.txt'):
    with open('.last_audio.txt', 'r') as f:
        audio_file = f.read().strip()
    print(f"Playing analyzed file: {audio_file}")
    display(Audio(audio_file))
else:
    print("âš ï¸  Could not find .last_audio.txt - file path not saved")

# 5. Display Result
print("\nğŸ“Š Displaying result...")
from IPython.display import Image
if os.path.exists('prediction_result.png'):
    display(Image('prediction_result.png'))
else:
    print("âŒ Result image not found")

print("\nâœ… Complete! Run again to test a different random sample.")
```

### Using Custom Audio File

```python
# Upload your audio file
from google.colab import files
uploaded = files.upload()

# Get filename
audio_file = list(uploaded.keys())[0]

# Run inference
!python inference.py --audio {audio_file}

# Display result
from IPython.display import Image, display
display(Image('prediction_result.png'))
```

### Command Line Options

```bash
# Use random sample (randomly selects a .wav file from sample_audio/)
python inference.py

# Use specific audio file
python inference.py --audio path/to/your/audio.wav

# Play audio automatically (works in Colab/Jupyter)
python inference.py --play

# Show debug information
python inference.py --debug

# Save result image
python inference.py --audio audio.wav --save
```

### Alternative: Automatic Audio Playback

```python
# Run inference with automatic audio playback
!python inference.py --play

# Display result
from IPython.display import Image
if os.path.exists('prediction_result.png'):
    display(Image('prediction_result.png'))
```

### Kaggle Notebook
[View on Kaggle](https://www.kaggle.com/code/mahad69/speech-emotion-recognition-ensemble)

## Repository Structure
```
speech-emotion-recognition-ensemble/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                          # Updated with inference instructions
â”œâ”€â”€ requirements.txt                   # Updated with inference deps
â”œâ”€â”€ inference.py                       # Main inference script
â”œâ”€â”€ config.yaml
â”œâ”€â”€ config_optimized.yaml
â”œâ”€â”€ final_config.yaml
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_initial_implementation.ipynb
â”‚   â”œâ”€â”€ 02_gpu_optimization.ipynb
â”‚   â””â”€â”€ 03_test_time_augmentation.ipynb
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ ANN_DL_Final_Report.pdf
â”‚   â””â”€â”€ ANN_DL_Report.md
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confidence_distribution.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ ensemble_model_contribution_weights.png
â”‚   â”œâ”€â”€ per-class_accuracy.png
â”‚   â”œâ”€â”€ results.json
â”‚   â”œâ”€â”€ top_20_most_important_features.png
â”‚   â””â”€â”€ training_history.png
â”œâ”€â”€ sample_audio/                      # Sample audio files
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ *.wav                          # Audio samples for testing
â””â”€â”€ src/
    â”œâ”€â”€ augmentation.py
    â””â”€â”€ models/
        â”œâ”€â”€ best_model.pth
        â””â”€â”€ final_model.pth
```

## Performance Metrics
| Model | Accuracy | Parameters | Training Time |
|-------|----------|------------|---------------|
| CNN | 62.01% | 1.1M | 15s/epoch |
| LSTM | 67.66% | 1.2M | 18s/epoch |
| Transformer | 45.07% | 1.0M | 20s/epoch |
| **Ensemble** | **66.74%** | **3.3M** | **19s/epoch** |

## Technical Highlights
- Multi-GPU training with DataParallel
- Mixed precision (FP16) computation
- CosineAnnealingWarmRestarts scheduler
- Test Time Augmentation
- Attention mechanisms in LSTM and Transformer

## Citation
If you use this code, please cite:
```bibtex
@misc{AhadImran2025speech,
  title={Speech Emotion Recognition using Ensemble Deep Learning},
  author={[Ahad Imran]},
  year={2025},
  school={[NUTECH]}
}
```

## ğŸ“„ License
This project is for educational purposes. MIT License.

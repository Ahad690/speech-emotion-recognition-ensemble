{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:12:39.030598Z","iopub.execute_input":"2025-10-26T19:12:39.030753Z","iopub.status.idle":"2025-10-26T19:12:40.107753Z","shell.execute_reply.started":"2025-10-26T19:12:39.030738Z","shell.execute_reply":"2025-10-26T19:12:40.107138Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# ============================================\n# CELL 1: Setup and Installation\n# ============================================","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CELL 1: Setup and Installation (FINAL VERSION)\n# ============================================\n\"\"\"\nSpeech Emotion Recognition System\nFor: Speech Processing & ANN/DL Course\nAuthor: Ahad Imran\n\"\"\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check what's already installed\nimport sys\nprint(f\"Python: {sys.version}\")\n\n# Check librosa\ntry:\n    import librosa\n    print(f\"âœ“ librosa {librosa.__version__}\")\nexcept:\n    print(\"Installing librosa...\")\n    !pip install -q librosa\n\n# Skip audiomentations - not critical, we have built-in augmentation\n\n# Import all required packages\nimport os\nimport gc\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple, Optional\nimport pickle\nimport json\nfrom collections import Counter\n\n# Audio processing\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport IPython.display as ipd\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchaudio\nimport torchaudio.transforms as T\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Visualization\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Set seeds\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(42)\n\n# Check GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nUsing device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\nprint(\"\\n Setup complete! Ready to proceed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:12:40.109461Z","iopub.execute_input":"2025-10-26T19:12:40.109805Z","iopub.status.idle":"2025-10-26T19:12:51.719282Z","shell.execute_reply.started":"2025-10-26T19:12:40.109786Z","shell.execute_reply":"2025-10-26T19:12:51.718420Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nâœ“ librosa 0.11.0\n\nUsing device: cuda\nGPU: Tesla T4\nMemory: 15.83 GB\n\n Setup complete! Ready to proceed.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# ============================================\n# CELL 1a: Setup for Dual T4 GPUs\n# ============================================","metadata":{}},{"cell_type":"code","source":"from torch.nn.parallel import DataParallel\n\n# Check available GPUs\nprint(f\"GPUs available: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n\nif torch.cuda.device_count() > 1:\n    print(\"\\nâœ… Multiple GPUs detected! Will use DataParallel for faster training.\")\nelse:\n    print(\"\\nâœ… Single GPU detected. Will proceed with standard training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:12:51.720098Z","iopub.execute_input":"2025-10-26T19:12:51.720748Z","iopub.status.idle":"2025-10-26T19:12:51.725696Z","shell.execute_reply.started":"2025-10-26T19:12:51.720722Z","shell.execute_reply":"2025-10-26T19:12:51.725103Z"}},"outputs":[{"name":"stdout","text":"GPUs available: 2\nGPU 0: Tesla T4\nMemory: 15.83 GB\nGPU 1: Tesla T4\nMemory: 15.83 GB\n\nâœ… Multiple GPUs detected! Will use DataParallel for faster training.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# ============================================\n# CELL 2: Download and Prepare Datasets\n# ============================================","metadata":{}},{"cell_type":"markdown","source":"To set up environment variables for the Kaggle API using Python, you can use the `os` module to assign your credentials directly in your script. This is especially useful when you donâ€™t want to rely on a `kaggle.json` file. Here's how to do it:\n\n---\n\n### ðŸ”‘ Step-by-Step: Set Kaggle API Key with `os.environ`\n\n```python\nimport os\n\n# Set your Kaggle credentials\nos.environ['KAGGLE_USERNAME'] = 'your_kaggle_username'\nos.environ['KAGGLE_KEY'] = 'your_kaggle_api_key'\n```\n\nReplace `'your_kaggle_username'` and `'your_kaggle_api_key'` with the actual values from your [Kaggle account settings](https://www.kaggle.com/settings).\n\n---\n\n### ðŸ“¦ Then You Can Download Datasets Like This\n\n```python\n!pip install kaggle\n\n# Example: Download Titanic dataset\n!kaggle competitions download -c titanic\n```\n\nThis will work in environments like Jupyter, Colab, or Kaggle Notebooks â€” as long as the API key is valid and you've accepted the competition rules (if required).\n\n## Usage Example\n```python\n# !kaggle competitions download -c titanic\n\n\n# import zipfile\n\n# with zipfile.ZipFile('/kaggle/working/titanic.zip', 'r') as zip_ref:\n#    zip_ref.extractall('/kaggle/working')\n```","metadata":{}},{"cell_type":"code","source":"\"\"\"\nUsing Kaggle datasets for emotion recognition\n\"\"\"\n\nimport os\nimport zipfile\nfrom pathlib import Path\n\n# Setup Kaggle API credentials using Secrets\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    \n    os.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"kaggle_username\")\n    os.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"kaggle_key\")\n    \n    print(\"âœ“ Kaggle API configured with secrets\")\n    api_available = True\nexcept Exception as e:\n    print(f\"âš ï¸ Kaggle secrets not found: {e}\")\n    print(\"Please add datasets via 'Add Data' button or configure secrets.\")\n    api_available = False\n\n# Create directory structure\nos.makedirs('/kaggle/working/data', exist_ok=True)\nos.makedirs('/kaggle/working/models', exist_ok=True)\nos.makedirs('/kaggle/working/results', exist_ok=True)\n\n# Check if datasets are already added via UI\ndatasets_found = False\nif os.path.exists('/kaggle/input/'):\n    input_datasets = os.listdir('/kaggle/input/')\n    if len(input_datasets) > 0:\n        print(\"Datasets found in /kaggle/input/:\")\n        for dataset in input_datasets:\n            print(f\"  âœ“ {dataset}\")\n        datasets_found = True\n        DATA_PATH = '/kaggle/input/'\n\n# Method 2: Download if not added via UI (only if API is available)\nif not datasets_found and api_available:  # <-- FIXED: Added api_available check\n    print(\"\\nNo datasets found in input. Downloading...\")\n    \n    # Only download if not already present\n    if not os.path.exists('/kaggle/working/ravdess'):\n        print(\"Downloading RAVDESS...\")\n        !kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio -p /kaggle/working --quiet\n        \n        if os.path.exists('/kaggle/working/ravdess-emotional-speech-audio.zip'):\n            with zipfile.ZipFile('/kaggle/working/ravdess-emotional-speech-audio.zip', 'r') as zip_ref:\n                zip_ref.extractall('/kaggle/working/ravdess')\n            os.remove('/kaggle/working/ravdess-emotional-speech-audio.zip')\n            print(\"âœ“ RAVDESS downloaded\")\n    \n    if not os.path.exists('/kaggle/working/tess'):\n        print(\"Downloading TESS...\")\n        !kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess -p /kaggle/working --quiet\n        \n        if os.path.exists('/kaggle/working/toronto-emotional-speech-set-tess.zip'):\n            with zipfile.ZipFile('/kaggle/working/toronto-emotional-speech-set-tess.zip', 'r') as zip_ref:\n                zip_ref.extractall('/kaggle/working/tess')\n            os.remove('/kaggle/working/toronto-emotional-speech-set-tess.zip')\n            print(\"âœ“ TESS downloaded\")\n    \n    if not os.path.exists('/kaggle/working/cremad'):\n        print(\"Downloading CREMA-D...\")\n        !kaggle datasets download -d ejlok1/cremad -p /kaggle/working --quiet\n        \n        if os.path.exists('/kaggle/working/cremad.zip'):\n            with zipfile.ZipFile('/kaggle/working/cremad.zip', 'r') as zip_ref:\n                zip_ref.extractall('/kaggle/working/cremad')\n            os.remove('/kaggle/working/cremad.zip')\n            print(\"âœ“ CREMA-D downloaded\")\n    \n    DATA_PATH = '/kaggle/working/'\n\nelif not datasets_found and not api_available:\n    print(\"\\nâš ï¸ No datasets found and API not configured.\")\n    print(\"Please either:\")\n    print(\"1. Add datasets using the 'Add Data' button, or\")\n    print(\"2. Configure Kaggle API secrets (kaggle_username and kaggle_key)\")\n    DATA_PATH = '/kaggle/working/'  # Set default path anyway\n\nelse:\n    # Datasets already found\n    pass\n\n# Verify final state\nprint(f\"\\nUsing DATA_PATH: {DATA_PATH}\")\nif os.path.exists(DATA_PATH):\n    contents = os.listdir(DATA_PATH)\n    if contents:\n        print(f\"Found {len(contents)} items in {DATA_PATH}\")\n    else:\n        print(\"âš ï¸ DATA_PATH is empty. Please add datasets.\")\n\nprint(\"\\n Setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:12:51.726536Z","iopub.execute_input":"2025-10-26T19:12:51.726800Z","iopub.status.idle":"2025-10-26T19:13:22.751951Z","shell.execute_reply.started":"2025-10-26T19:12:51.726777Z","shell.execute_reply":"2025-10-26T19:13:22.751040Z"}},"outputs":[{"name":"stdout","text":"âœ“ Kaggle API configured with secrets\n\nNo datasets found in input. Downloading...\nDownloading RAVDESS...\nDataset URL: https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio\nLicense(s): CC-BY-NC-SA-4.0\nâœ“ RAVDESS downloaded\nDownloading TESS...\nDataset URL: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess\nLicense(s): Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)\nâœ“ TESS downloaded\nDownloading CREMA-D...\nDataset URL: https://www.kaggle.com/datasets/ejlok1/cremad\nLicense(s): ODC Attribution License (ODC-By)\nâœ“ CREMA-D downloaded\n\nUsing DATA_PATH: /kaggle/working/\nFound 7 items in /kaggle/working/\n\n Setup complete!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Quick Debug: Check what's actually in the directories\n# DEBUG CELL: Check dataset structure\nimport os\n\nfor dataset in ['ravdess', 'tess', 'cremad']:\n    path = f'/kaggle/working/{dataset}'\n    if os.path.exists(path):\n        print(f\"\\n{dataset.upper()} structure:\")\n        for root, dirs, files in os.walk(path):\n            level = root.replace(path, '').count(os.sep)\n            if level < 3:  # Only show first 3 levels\n                indent = ' ' * 2 * level\n                print(f\"{indent}{os.path.basename(root)}/\")\n                if level < 2:\n                    wav_files = [f for f in files if f.endswith('.wav')]\n                    if wav_files:\n                        print(f\"{indent}  [{len(wav_files)} .wav files]\")\n                        print(f\"{indent}  Sample: {wav_files[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.753119Z","iopub.execute_input":"2025-10-26T19:13:22.753854Z","iopub.status.idle":"2025-10-26T19:13:22.776463Z","shell.execute_reply.started":"2025-10-26T19:13:22.753827Z","shell.execute_reply":"2025-10-26T19:13:22.775829Z"}},"outputs":[{"name":"stdout","text":"\nRAVDESS structure:\nravdess/\n  Actor_16/\n    [60 .wav files]\n    Sample: 03-01-03-02-02-02-16.wav\n  Actor_02/\n    [60 .wav files]\n    Sample: 03-01-03-01-01-02-02.wav\n  Actor_01/\n    [60 .wav files]\n    Sample: 03-01-02-02-01-02-01.wav\n  Actor_10/\n    [60 .wav files]\n    Sample: 03-01-05-01-01-02-10.wav\n  Actor_12/\n    [60 .wav files]\n    Sample: 03-01-05-02-01-01-12.wav\n  Actor_15/\n    [60 .wav files]\n    Sample: 03-01-07-01-01-02-15.wav\n  Actor_13/\n    [60 .wav files]\n    Sample: 03-01-01-01-01-01-13.wav\n  Actor_05/\n    [60 .wav files]\n    Sample: 03-01-02-02-01-01-05.wav\n  Actor_14/\n    [60 .wav files]\n    Sample: 03-01-07-01-01-01-14.wav\n  Actor_07/\n    [60 .wav files]\n    Sample: 03-01-04-02-02-01-07.wav\n  Actor_06/\n    [60 .wav files]\n    Sample: 03-01-08-01-01-01-06.wav\n  Actor_03/\n    [60 .wav files]\n    Sample: 03-01-03-01-02-01-03.wav\n  Actor_08/\n    [60 .wav files]\n    Sample: 03-01-03-01-02-01-08.wav\n  Actor_09/\n    [60 .wav files]\n    Sample: 03-01-05-01-01-02-09.wav\n  Actor_21/\n    [60 .wav files]\n    Sample: 03-01-02-02-01-02-21.wav\n  Actor_18/\n    [60 .wav files]\n    Sample: 03-01-05-01-01-02-18.wav\n  Actor_24/\n    [60 .wav files]\n    Sample: 03-01-06-01-02-01-24.wav\n  Actor_11/\n    [60 .wav files]\n    Sample: 03-01-01-01-02-01-11.wav\n  Actor_22/\n    [60 .wav files]\n    Sample: 03-01-08-01-01-02-22.wav\n  audio_speech_actors_01-24/\n    Actor_16/\n    Actor_02/\n    Actor_01/\n    Actor_10/\n    Actor_12/\n    Actor_15/\n    Actor_13/\n    Actor_05/\n    Actor_14/\n    Actor_07/\n    Actor_06/\n    Actor_03/\n    Actor_08/\n    Actor_09/\n    Actor_21/\n    Actor_18/\n    Actor_24/\n    Actor_11/\n    Actor_22/\n    Actor_19/\n    Actor_23/\n    Actor_17/\n    Actor_20/\n    Actor_04/\n  Actor_19/\n    [60 .wav files]\n    Sample: 03-01-08-02-02-02-19.wav\n  Actor_23/\n    [60 .wav files]\n    Sample: 03-01-07-02-02-01-23.wav\n  Actor_17/\n    [60 .wav files]\n    Sample: 03-01-08-01-02-02-17.wav\n  Actor_20/\n    [60 .wav files]\n    Sample: 03-01-04-02-02-01-20.wav\n  Actor_04/\n    [60 .wav files]\n    Sample: 03-01-08-02-01-01-04.wav\n\nTESS structure:\ntess/\n  TESS Toronto emotional speech set data/\n    OAF_Pleasant_surprise/\n    YAF_fear/\n    YAF_angry/\n    YAF_pleasant_surprised/\n    YAF_happy/\n    OAF_disgust/\n    OAF_happy/\n    OAF_Fear/\n    OAF_neutral/\n    YAF_neutral/\n    YAF_sad/\n    YAF_disgust/\n    OAF_Sad/\n    OAF_angry/\n  tess toronto emotional speech set data/\n    TESS Toronto emotional speech set data/\n\nCREMAD structure:\ncremad/\n  AudioWAV/\n    [7442 .wav files]\n    Sample: 1021_TSI_DIS_XX.wav\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# ============================================\n# CELL 3: Configuration\n# ============================================","metadata":{}},{"cell_type":"code","source":"# ============================================\n# IMPROVED CELL 3: Better Configuration\n# ============================================\nclass Config:\n    \"\"\"Improved configuration for better accuracy\"\"\"\n    \n    # Project\n    project_name = \"Speech Emotion Recognition\"\n    \n    # Data\n    sample_rate = 16000\n    duration = 3.0\n    n_classes = 7\n    \n    # Features\n    n_mfcc = 40\n    n_mels = 128\n    n_fft = 2048\n    hop_length = 512\n    \n    # Data splits\n    train_size = 0.7\n    val_size = 0.15\n    test_size = 0.15\n    \n    # IMPROVED Training parameters\n    batch_size = 64\n    epochs = 150  # Increased from 100\n    learning_rate = 5e-4  # Reduced from 1e-3\n    early_stopping_patience = 15  # Increased from 10\n    \n    # Model\n    model_type = 'ensemble'\n    dropout = 0.4  # Increased from 0.3 for better regularization\n    \n    # IMPROVED Augmentation\n    use_augmentation = True\n    augment_prob = 0.7  # Increased from 0.5\n\n     # Paths\n    data_path = '/kaggle/working/'  # Or '/kaggle/input/' if using Add Data\n    save_path = '/kaggle/working/'\n    \nconfig = Config()\nprint(\"âœ… Configuration loaded\")\nprint(f\"Model type: {config.model_type}\")\nprint(f\"Batch size: {config.batch_size}\")\nprint(f\"Epochs: {config.epochs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:18:28.419061Z","iopub.execute_input":"2025-10-26T19:18:28.419858Z","iopub.status.idle":"2025-10-26T19:18:28.425517Z","shell.execute_reply.started":"2025-10-26T19:18:28.419824Z","shell.execute_reply":"2025-10-26T19:18:28.424883Z"}},"outputs":[{"name":"stdout","text":"âœ… Configuration loaded\nModel type: ensemble\nBatch size: 64\nEpochs: 150\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# ============================================\n# CELL 4: Dataset Class with Memory Optimization\n# ============================================","metadata":{}},{"cell_type":"code","source":"class EmotionDataset(Dataset):\n    \"\"\"\n    Memory-efficient dataset for Kaggle\n    \"\"\"\n    \n    def __init__(\n        self, \n        file_paths: List[str],\n        labels: List[int],\n        config: Config,\n        transform=None,\n        augment=False\n    ):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.config = config\n        self.transform = transform\n        self.augment = augment\n        \n        # Pre-calculate fixed length\n        self.target_length = int(config.sample_rate * config.duration)\n        \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        # Load audio on-demand to save memory\n        audio_path = self.file_paths[idx]\n        label = self.labels[idx]\n        \n        try:\n            # Load audio\n            waveform, sr = librosa.load(audio_path, sr=self.config.sample_rate, mono=True)\n            \n            # Pad or truncate\n            if len(waveform) > self.target_length:\n                waveform = waveform[:self.target_length]\n            else:\n                waveform = np.pad(waveform, (0, self.target_length - len(waveform)))\n            \n            # Convert to tensor\n            waveform = torch.FloatTensor(waveform).unsqueeze(0)\n            \n            # Apply augmentation\n            if self.augment and random.random() < self.config.augment_prob:\n                waveform = self.augment_audio(waveform)\n            \n            # Extract features\n            features = self.extract_features(waveform)\n            \n            return features, label\n            \n        except Exception as e:\n            print(f\"Error loading {audio_path}: {e}\")\n            # Return zeros if error\n            return torch.zeros((self.config.n_mels, 94)), label\n    \n    def augment_audio(self, waveform):\n        \"\"\"Simple augmentation\"\"\"\n        # Add noise\n        if random.random() > 0.5:\n            noise = torch.randn_like(waveform) * 0.005\n            waveform = waveform + noise\n        \n        # Time shift\n        if random.random() > 0.5:\n            shift = int(random.uniform(-0.1, 0.1) * waveform.shape[1])\n            waveform = torch.roll(waveform, shift, dims=1)\n        \n        return waveform\n    \n    def extract_features(self, waveform):\n        \"\"\"Extract mel-spectrogram features\"\"\"\n        mel_transform = T.MelSpectrogram(\n            sample_rate=self.config.sample_rate,\n            n_mels=self.config.n_mels,\n            n_fft=self.config.n_fft,\n            hop_length=self.config.hop_length\n        )\n        \n        mel_spec = mel_transform(waveform)\n        mel_spec_db = T.AmplitudeToDB()(mel_spec)\n        \n        return mel_spec_db.squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:18:33.023470Z","iopub.execute_input":"2025-10-26T19:18:33.023764Z","iopub.status.idle":"2025-10-26T19:18:33.032906Z","shell.execute_reply.started":"2025-10-26T19:18:33.023743Z","shell.execute_reply":"2025-10-26T19:18:33.032281Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# ============================================\n# CELL 5: Data Loading and Preparation\n# ============================================","metadata":{}},{"cell_type":"code","source":"def prepare_data(config):\n    \"\"\"\n    Load and prepare datasets with correct paths\n    \"\"\"\n    all_files = []\n    all_labels = []\n    \n    # Emotion mapping\n    emotion_map = {\n        'neutral': 0, 'calm': 0,  # Merge calm into neutral\n        'happy': 1, 'sad': 2, 'angry': 3,\n        'fearful': 4, 'fear': 4,  # Handle variations\n        'disgust': 5, 'surprised': 6, 'surprise': 6\n    }\n    \n    base_path = Path('/kaggle/working')\n    \n    # RAVDESS dataset - files are in Actor_XX folders\n    ravdess_path = base_path / 'ravdess'\n    if ravdess_path.exists():\n        print(\"Loading RAVDESS dataset...\")\n        # Look for Actor folders\n        for actor_folder in ravdess_path.glob('Actor_*'):\n            if actor_folder.is_dir():\n                for audio_file in actor_folder.glob('*.wav'):\n                    # Parse RAVDESS filename (03-01-06-01-02-01-12.wav)\n                    parts = audio_file.stem.split('-')\n                    if len(parts) >= 3:\n                        emotion_code = int(parts[2])\n                        ravdess_emotions = {\n                            1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',\n                            5: 'angry', 6: 'fear', 7: 'disgust', 8: 'surprise'\n                        }\n                        if emotion_code in ravdess_emotions:\n                            emotion = ravdess_emotions[emotion_code]\n                            all_files.append(str(audio_file))\n                            all_labels.append(emotion_map[emotion])\n        print(f\"  Found {len(all_files)} RAVDESS files\")\n    \n    # TESS dataset - files are in emotion-specific folders\n    tess_path = base_path / 'tess' / 'TESS Toronto emotional speech set data'\n    if tess_path.exists():\n        print(\"Loading TESS dataset...\")\n        initial_count = len(all_files)\n        \n        # TESS has folders like OAF_angry, YAF_happy, etc.\n        for emotion_folder in tess_path.glob('*'):\n            if emotion_folder.is_dir():\n                folder_name = emotion_folder.name.lower()\n                \n                # Extract emotion from folder name\n                if 'angry' in folder_name:\n                    emotion = 'angry'\n                elif 'disgust' in folder_name:\n                    emotion = 'disgust'\n                elif 'fear' in folder_name:\n                    emotion = 'fear'\n                elif 'happy' in folder_name:\n                    emotion = 'happy'\n                elif 'sad' in folder_name:\n                    emotion = 'sad'\n                elif 'neutral' in folder_name:\n                    emotion = 'neutral'\n                elif 'surprise' in folder_name or 'surprised' in folder_name:\n                    emotion = 'surprise'\n                else:\n                    continue  # Skip unknown folders\n                \n                # Add all wav files from this emotion folder\n                for audio_file in emotion_folder.glob('*.wav'):\n                    all_files.append(str(audio_file))\n                    all_labels.append(emotion_map[emotion])\n        \n        print(f\"  Found {len(all_files) - initial_count} TESS files\")\n    \n    # CREMA-D dataset - files are in AudioWAV folder\n    cremad_path = base_path / 'cremad' / 'AudioWAV'\n    if cremad_path.exists():\n        print(\"Loading CREMA-D dataset...\")\n        initial_count = len(all_files)\n        \n        for audio_file in cremad_path.glob('*.wav'):\n            # CREMA-D format: 1001_DFA_ANG_XX.wav\n            filename = audio_file.stem\n            if '_' in filename:\n                parts = filename.split('_')\n                if len(parts) >= 3:\n                    emotion_code = parts[2]\n                    cremad_emotions = {\n                        'ANG': 'angry', 'DIS': 'disgust', 'FEA': 'fear',\n                        'HAP': 'happy', 'NEU': 'neutral', 'SAD': 'sad'\n                    }\n                    if emotion_code in cremad_emotions:\n                        emotion = cremad_emotions[emotion_code]\n                        all_files.append(str(audio_file))\n                        all_labels.append(emotion_map[emotion])\n        \n        print(f\"  Found {len(all_files) - initial_count} CREMA-D files\")\n    \n    # Summary\n    if len(all_files) == 0:\n        print(\"\\nâš ï¸ No audio files found. Please check dataset paths.\")\n        print(\"Creating synthetic data for testing...\")\n        for i in range(100):\n            all_files.append(f\"dummy_{i}.wav\")\n            all_labels.append(random.randint(0, 6))\n    else:\n        print(f\"\\nâœ… Successfully loaded all datasets!\")\n    \n    print(f\"Total samples: {len(all_files)}\")\n    \n    # Show label distribution\n    label_counts = Counter(all_labels)\n    emotion_names = {v: k for k, v in emotion_map.items()}\n    print(\"\\nEmotion distribution:\")\n    for label, count in sorted(label_counts.items()):\n        emotion_name = [k for k, v in emotion_map.items() if v == label][0]\n        print(f\"  {emotion_name}: {count} samples\")\n    \n    # Update number of classes\n    config.n_classes = len(set(all_labels))\n    print(f\"\\nNumber of emotion classes: {config.n_classes}\")\n    \n    return all_files, all_labels\n\n# Load data with the fixed function\nfile_paths, labels = prepare_data(config)\n\n# Split data\nX_temp, X_test, y_temp, y_test = train_test_split(\n    file_paths, labels, test_size=config.test_size, \n    stratify=labels, random_state=42\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=config.val_size/(1-config.test_size),\n    stratify=y_temp, random_state=42\n)\n\nprint(f\"\\nDataset splits:\")\nprint(f\"  Train: {len(X_train)} samples\")\nprint(f\"  Val: {len(X_val)} samples\")\nprint(f\"  Test: {len(X_test)} samples\")","metadata":{"execution":{"iopub.status.busy":"2025-10-26T19:18:37.831254Z","iopub.execute_input":"2025-10-26T19:18:37.831555Z","iopub.status.idle":"2025-10-26T19:18:37.921139Z","shell.execute_reply.started":"2025-10-26T19:18:37.831524Z","shell.execute_reply":"2025-10-26T19:18:37.920534Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading RAVDESS dataset...\n  Found 1440 RAVDESS files\nLoading TESS dataset...\n  Found 2800 TESS files\nLoading CREMA-D dataset...\n  Found 7442 CREMA-D files\n\nâœ… Successfully loaded all datasets!\nTotal samples: 11682\n\nEmotion distribution:\n  neutral: 1775 samples\n  happy: 1863 samples\n  sad: 1863 samples\n  angry: 1863 samples\n  fearful: 1863 samples\n  disgust: 1863 samples\n  surprised: 592 samples\n\nNumber of emotion classes: 7\n\nDataset splits:\n  Train: 8176 samples\n  Val: 1753 samples\n  Test: 1753 samples\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# ============================================\n# CELL 6: Model Architectures\n# ============================================","metadata":{}},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    \"\"\"CNN for emotion recognition\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        \n        # Global pooling\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(64, config.n_classes)\n        )\n        \n    def forward(self, x):\n        # Add channel dimension if needed\n        if x.dim() == 3:\n            x = x.unsqueeze(1)\n        \n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.global_pool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x\n\n\nclass LSTMModel(nn.Module):\n    \"\"\"LSTM for emotion recognition\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.lstm = nn.LSTM(\n            input_size=config.n_mels,\n            hidden_size=128,\n            num_layers=2,\n            batch_first=True,\n            dropout=config.dropout,\n            bidirectional=True\n        )\n        \n        self.attention = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.Tanh(),\n            nn.Linear(128, 1)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(128, config.n_classes)\n        )\n        \n    def forward(self, x):\n        # Reshape for LSTM (batch, time, features)\n        if x.dim() == 4:\n            x = x.squeeze(1)\n        x = x.transpose(1, 2)\n        \n        lstm_out, _ = self.lstm(x)\n        \n        # Attention\n        attn_weights = self.attention(lstm_out)\n        attn_weights = F.softmax(attn_weights, dim=1)\n        attended = torch.sum(lstm_out * attn_weights, dim=1)\n        \n        return self.classifier(attended)\n\n\nclass TransformerModel(nn.Module):\n    \"\"\"Transformer for emotion recognition\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.input_projection = nn.Linear(config.n_mels, 256)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=256,\n            nhead=8,\n            dim_feedforward=512,\n            dropout=config.dropout,\n            batch_first=True\n        )\n        \n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(128, config.n_classes)\n        )\n        \n    def forward(self, x):\n        # Reshape (batch, time, features)\n        if x.dim() == 4:\n            x = x.squeeze(1)\n        x = x.transpose(1, 2)\n        \n        x = self.input_projection(x)\n        x = self.transformer(x)\n        \n        # Global average pooling\n        x = x.mean(dim=1)\n        \n        return self.classifier(x)\n\n\nclass EnsembleModel(nn.Module):\n    \"\"\"Ensemble of multiple models\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.cnn = CNNModel(config)\n        self.lstm = LSTMModel(config)\n        self.transformer = TransformerModel(config)\n        \n        # Learnable weights for ensemble\n        self.weights = nn.Parameter(torch.ones(3) / 3)\n        \n    def forward(self, x):\n        cnn_out = self.cnn(x)\n        lstm_out = self.lstm(x)\n        transformer_out = self.transformer(x)\n        \n        # Weighted average\n        w = F.softmax(self.weights, dim=0)\n        output = w[0] * cnn_out + w[1] * lstm_out + w[2] * transformer_out\n        \n        return output\n\n    def augment_audio(self, waveform):\n        \"\"\"Enhanced augmentation for better generalization\"\"\"\n        \n        # Apply multiple augmentations\n        augmentations_applied = 0\n        \n        # 1. Add noise (30% chance)\n        if random.random() > 0.7:\n            noise_factor = random.uniform(0.002, 0.01)\n            noise = torch.randn_like(waveform) * noise_factor\n            waveform = waveform + noise\n            augmentations_applied += 1\n        \n        # 2. Time shift (30% chance)\n        if random.random() > 0.7:\n            shift = int(random.uniform(-0.2, 0.2) * waveform.shape[1])\n            waveform = torch.roll(waveform, shift, dims=1)\n            augmentations_applied += 1\n        \n        # 3. Speed change simulation (30% chance)\n        if random.random() > 0.7:\n            speed_factor = random.uniform(0.9, 1.1)\n            # Simple speed change by resampling\n            old_length = waveform.shape[1]\n            new_length = int(old_length * speed_factor)\n            indices = torch.linspace(0, old_length - 1, new_length).long()\n            waveform = waveform[:, indices]\n            # Pad or truncate back to original length\n            if waveform.shape[1] > old_length:\n                waveform = waveform[:, :old_length]\n            else:\n                padding = old_length - waveform.shape[1]\n                waveform = torch.nn.functional.pad(waveform, (0, padding))\n            augmentations_applied += 1\n        \n        # 4. Volume change (30% chance)\n        if random.random() > 0.7:\n            volume_factor = random.uniform(0.7, 1.3)\n            waveform = waveform * volume_factor\n            augmentations_applied += 1\n        \n        return waveform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:19:22.075696Z","iopub.execute_input":"2025-10-26T19:19:22.076378Z","iopub.status.idle":"2025-10-26T19:19:22.093512Z","shell.execute_reply.started":"2025-10-26T19:19:22.076342Z","shell.execute_reply":"2025-10-26T19:19:22.092876Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# ============================================\n# CELL 7: Training Functions\n# ============================================","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CELL 7: IMPROVED Trainer with Better Scheduling\n# ============================================\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport gc\n\nclass Trainer:\n    \"\"\"Training manager with improved learning rate scheduling\"\"\"\n    \n    def __init__(self, model, config, device):\n        self.model = model.to(device)\n        self.config = config\n        self.device = device\n        \n        self.criterion = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n        \n        # IMPROVED: Use Cosine Annealing with Warm Restarts for better convergence\n        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.optimizer, \n            T_0=10,  # Restart every 10 epochs\n            T_mult=2,  # Double the restart interval each time\n            eta_min=1e-6  # Minimum learning rate\n        )\n        \n        # Mixed precision for T4\n        self.scaler = GradScaler()\n        \n        self.train_losses = []\n        self.val_losses = []\n        self.train_accs = []\n        self.val_accs = []\n        \n    def train_epoch(self, dataloader):\n        self.model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch_idx, (features, labels) in enumerate(tqdm(dataloader, desc=\"Training\")):\n            features = features.to(self.device)\n            labels = labels.to(self.device)\n            \n            self.optimizer.zero_grad()\n            \n            # Mixed precision training\n            with autocast():\n                outputs = self.model(features)\n                loss = self.criterion(outputs, labels)\n            \n            # Scaled backprop for mixed precision\n            self.scaler.scale(loss).backward()\n            \n            # Gradient clipping\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n            \n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n            \n            total_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            # Clear cache periodically\n            if batch_idx % 10 == 0:\n                torch.cuda.empty_cache()\n        \n        return total_loss / len(dataloader), 100. * correct / total\n    \n    def validate(self, dataloader):\n        self.model.eval()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            with autocast():\n                for features, labels in tqdm(dataloader, desc=\"Validation\"):\n                    features = features.to(self.device)\n                    labels = labels.to(self.device)\n                    \n                    outputs = self.model(features)\n                    loss = self.criterion(outputs, labels)\n                    \n                    total_loss += loss.item()\n                    _, predicted = outputs.max(1)\n                    total += labels.size(0)\n                    correct += predicted.eq(labels).sum().item()\n        \n        return total_loss / len(dataloader), 100. * correct / total\n    \n    def fit(self, train_loader, val_loader):\n        best_val_acc = 0\n        patience_counter = 0\n        \n        for epoch in range(self.config.epochs):\n            print(f\"\\nEpoch {epoch+1}/{self.config.epochs}\")\n            \n            # Training\n            train_loss, train_acc = self.train_epoch(train_loader)\n            self.train_losses.append(train_loss)\n            self.train_accs.append(train_acc)\n            \n            # Validation\n            val_loss, val_acc = self.validate(val_loader)\n            self.val_losses.append(val_loss)\n            self.val_accs.append(val_acc)\n            \n            # IMPROVED: Step scheduler every epoch (for CosineAnnealingWarmRestarts)\n            self.scheduler.step()\n            current_lr = self.scheduler.get_last_lr()[0]\n            \n            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n            print(f\"Learning Rate: {current_lr:.6f}\")\n            \n            # Save best model\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_acc': val_acc,\n                    'config': self.config\n                }, '/kaggle/working/best_model.pth')\n                patience_counter = 0\n                print(f\"âœ“ Saved best model with {val_acc:.2f}% accuracy\")\n            else:\n                patience_counter += 1\n            \n            # Early stopping\n            if patience_counter >= self.config.early_stopping_patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n            \n            # Memory cleanup\n            gc.collect()\n            torch.cuda.empty_cache()\n        \n        return self.model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:19:29.456611Z","iopub.execute_input":"2025-10-26T19:19:29.457216Z","iopub.status.idle":"2025-10-26T19:19:29.472530Z","shell.execute_reply.started":"2025-10-26T19:19:29.457193Z","shell.execute_reply":"2025-10-26T19:19:29.471651Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# ============================================\n# CELL 8: Create DataLoaders\n# ============================================","metadata":{}},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = EmotionDataset(X_train, y_train, config, augment=True)\nval_dataset = EmotionDataset(X_val, y_val, config, augment=False)\ntest_dataset = EmotionDataset(X_test, y_test, config, augment=False)\n\n# Create dataloaders with num_workers=0 for Kaggle\ntrain_loader = DataLoader(\n    train_dataset, batch_size=config.batch_size, \n    shuffle=True, num_workers=0, pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, batch_size=config.batch_size, \n    shuffle=False, num_workers=0, pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=config.batch_size, \n    shuffle=False, num_workers=0, pin_memory=True\n)\n\nprint(f\"DataLoaders created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:19:37.010253Z","iopub.execute_input":"2025-10-26T19:19:37.010560Z","iopub.status.idle":"2025-10-26T19:19:37.016635Z","shell.execute_reply.started":"2025-10-26T19:19:37.010539Z","shell.execute_reply":"2025-10-26T19:19:37.015945Z"}},"outputs":[{"name":"stdout","text":"DataLoaders created successfully!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# ============================================\n# CELL 9: Train Model with DataParallel\n# ============================================","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CELL 9a: Train Individual Models for Comparison\n# ============================================\n\nresults = {}\n\n# Train CNN only\nprint(\"Training CNN model...\")\nconfig.model_type = 'cnn'\nconfig.epochs = 50\ncnn_model = CNNModel(config)\ncnn_trainer = Trainer(cnn_model, config, device)\ncnn_trainer.fit(train_loader, val_loader)\nresults['CNN'] = max(cnn_trainer.val_accs)\n\n# Train LSTM only\nprint(\"\\nTraining LSTM model...\")\nconfig.model_type = 'lstm'\nlstm_model = LSTMModel(config)\nlstm_trainer = Trainer(lstm_model, config, device)\nlstm_trainer.fit(train_loader, val_loader)\nresults['LSTM'] = max(lstm_trainer.val_accs)\n\n# Train Transformer only\nprint(\"\\nTraining Transformer model...\")\nconfig.model_type = 'transformer'\ntransformer_model = TransformerModel(config)\ntransformer_trainer = Trainer(transformer_model, config, device)\ntransformer_trainer.fit(train_loader, val_loader)\nresults['Transformer'] = max(transformer_trainer.val_accs)\n\nprint(\"\\nIndividual Model Results:\")\nfor model_name, acc in results.items():\n    print(f\"  {model_name}: {acc:.2f}%\")\n\n# Now train ensemble\nprint(\"\\nTraining Ensemble model...\")\nconfig.model_type = 'ensemble'\nconfig.epochs = 100\n# Continue with original CELL 9...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:19:41.583257Z","iopub.execute_input":"2025-10-26T19:19:41.583833Z","iopub.status.idle":"2025-10-26T19:23:03.982386Z","shell.execute_reply.started":"2025-10-26T19:19:41.583799Z","shell.execute_reply":"2025-10-26T19:23:03.981329Z"}},"outputs":[{"name":"stdout","text":"Training CNN model...\n\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:54<00:00,  2.33it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7324, Train Acc: 28.96%\nVal Loss: 1.5430, Val Acc: 36.28%\nLearning Rate: 0.000488\nâœ“ Saved best model with 36.28% accuracy\n\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:37<00:00,  3.39it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5942, Train Acc: 34.21%\nVal Loss: 1.7762, Val Acc: 27.44%\nLearning Rate: 0.000452\n\nEpoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:37<00:00,  3.38it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5504, Train Acc: 35.42%\nVal Loss: 1.4692, Val Acc: 39.53%\nLearning Rate: 0.000397\nâœ“ Saved best model with 39.53% accuracy\n\nEpoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:37<00:00,  3.39it/s]\nValidation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5025, Train Acc: 37.63%\nVal Loss: 1.4117, Val Acc: 41.64%\nLearning Rate: 0.000328\nâœ“ Saved best model with 41.64% accuracy\n\nEpoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|â–         | 5/128 [00:01<00:40,  3.04it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2551331451.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcnn_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcnn_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcnn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CNN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2746161327.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/2746161327.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/919569656.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Load audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Pad or truncate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# Final cleanup for dtype and contiguity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_mono\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mto_mono\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \"\"\"\n\u001b[1;32m    504\u001b[0m     \u001b[0;31m# Validate the buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/util/utils.py\u001b[0m in \u001b[0;36mvalid_audio\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    295\u001b[0m         )\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Audio buffer is not finite everywhere\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# ============================================\n# CELL 9: Train Model (COMPLETE VERSION)\n# ============================================\n\n# Select model based on config\nif config.model_type == 'cnn':\n    model = CNNModel(config)\nelif config.model_type == 'lstm':\n    model = LSTMModel(config)\nelif config.model_type == 'transformer':\n    model = TransformerModel(config)\nelse:  # ensemble\n    model = EnsembleModel(config)\n\n# Use DataParallel if multiple GPUs available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = DataParallel(model)\n\nprint(f\"Model: {config.model_type}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Create trainer and actually train the model\ntrainer = Trainer(model, config, device)\n\n# THIS IS THE IMPORTANT PART - Actually run training!\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting Training...\")\nprint(\"=\"*50)\n\ntrained_model = trainer.fit(train_loader, val_loader)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training Complete!\")\nprint(\"=\"*50)\n\n# Verify the model was saved\nimport os\nif os.path.exists('/kaggle/working/best_model.pth'):\n    print(\"âœ… Model saved successfully!\")\n    file_size = os.path.getsize('/kaggle/working/best_model.pth') / (1024*1024)\n    print(f\"Model file size: {file_size:.2f} MB\")\nelse:\n    print(\"âš ï¸ Model file not found. Training may have failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.874702Z","iopub.status.idle":"2025-10-26T19:13:22.874930Z","shell.execute_reply.started":"2025-10-26T19:13:22.874819Z","shell.execute_reply":"2025-10-26T19:13:22.874828Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 10: Evaluation and Visualization\n# ============================================","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, test_loader, device):\n    \"\"\"Comprehensive model evaluation\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for features, labels in tqdm(test_loader, desc=\"Testing\"):\n            features = features.to(device)\n            outputs = model(features)\n            probs = F.softmax(outputs, dim=1)\n            _, predicted = outputs.max(1)\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.numpy())\n            all_probs.extend(probs.cpu().numpy())\n    \n    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n\n# Load best model - FIXED for PyTorch 2.6\ncheckpoint = torch.load('/kaggle/working/best_model.pth', weights_only=False)  # <-- Added weights_only=False\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"âœ… Loaded best model from epoch {checkpoint['epoch']} with {checkpoint['val_acc']:.2f}% validation accuracy\")\n\n# Evaluate on test set\nprint(\"\\nEvaluating on test set...\")\npreds, labels, probs = evaluate_model(model, test_loader, device)\n\n# Calculate metrics\naccuracy = accuracy_score(labels, preds)\nprint(f\"\\nðŸŽ¯ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n\n# Classification report\nemotion_names = ['neutral', 'happy', 'sad', 'angry', 'fear', 'disgust', 'surprise']\nprint(\"\\n\" + \"=\"*60)\nprint(\"Classification Report:\")\nprint(\"=\"*60)\nprint(classification_report(labels, preds, target_names=emotion_names[:config.n_classes], digits=3))\n\n# Confusion Matrix\ncm = confusion_matrix(labels, preds)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Confusion Matrix:\")\nprint(\"=\"*60)\nprint(cm)\n\n# Calculate per-class accuracy\nper_class_acc = cm.diagonal() / cm.sum(axis=1)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Per-Class Accuracy:\")\nprint(\"=\"*60)\nfor i, emotion in enumerate(emotion_names[:config.n_classes]):\n    if i < len(per_class_acc):\n        print(f\"  {emotion:10s}: {per_class_acc[i]:.3f} ({per_class_acc[i]*100:.1f}%)\")\n\nprint(\"\\nâœ… Evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.875650Z","iopub.status.idle":"2025-10-26T19:13:22.875872Z","shell.execute_reply.started":"2025-10-26T19:13:22.875774Z","shell.execute_reply":"2025-10-26T19:13:22.875784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 11: Advanced Visualizations\n# ============================================","metadata":{}},{"cell_type":"code","source":"# 1. Training History\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('Loss', 'Accuracy')\n)\n\nfig.add_trace(\n    go.Scatter(y=trainer.train_losses, name='Train Loss', mode='lines'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(y=trainer.val_losses, name='Val Loss', mode='lines'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(y=trainer.train_accs, name='Train Acc', mode='lines'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(y=trainer.val_accs, name='Val Acc', mode='lines'),\n    row=1, col=2\n)\n\nfig.update_layout(height=400, title_text=\"Training History\")\nfig.show()\n\n# 2. Confusion Matrix Heatmap\nfig = px.imshow(\n    cm,\n    labels=dict(x=\"Predicted\", y=\"True\", color=\"Count\"),\n    x=emotion_names[:config.n_classes],\n    y=emotion_names[:config.n_classes],\n    title=\"Confusion Matrix\",\n    color_continuous_scale=\"Blues\",\n    text_auto=True\n)\nfig.update_layout(width=600, height=500)\nfig.show()\n\n# 3. Per-class Performance\nper_class_acc = cm.diagonal() / cm.sum(axis=1)\nfig = go.Figure(data=[\n    go.Bar(x=emotion_names[:config.n_classes], y=per_class_acc)\n])\nfig.update_layout(\n    title=\"Per-Class Accuracy\",\n    xaxis_title=\"Emotion\",\n    yaxis_title=\"Accuracy\",\n    yaxis_range=[0, 1]\n)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.877024Z","iopub.status.idle":"2025-10-26T19:13:22.877228Z","shell.execute_reply.started":"2025-10-26T19:13:22.877132Z","shell.execute_reply":"2025-10-26T19:13:22.877140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 12: Feature Importance Analysis\n# ============================================","metadata":{}},{"cell_type":"code","source":"def extract_features_classical(file_paths, config):\n    \"\"\"Extract features for classical ML\"\"\"\n    features = []\n    \n    for path in tqdm(file_paths[:100], desc=\"Extracting features\"):  # Limit for demo\n        try:\n            y, sr = librosa.load(path, sr=config.sample_rate)\n            \n            # MFCC\n            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=config.n_mfcc)\n            mfcc_mean = np.mean(mfcc, axis=1)\n            mfcc_std = np.std(mfcc, axis=1)\n            \n            # Chroma\n            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n            chroma_mean = np.mean(chroma, axis=1)\n            chroma_std = np.std(chroma, axis=1)\n            \n            # Spectral features\n            spec_cent = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n            spec_bw = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n            rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n            zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n            \n            # Combine features\n            feature_vector = np.hstack([\n                mfcc_mean, mfcc_std,\n                chroma_mean, chroma_std,\n                spec_cent, spec_bw, rolloff, zcr\n            ])\n            \n            features.append(feature_vector)\n        except:\n            features.append(np.zeros(104))  # Default feature size\n    \n    return np.array(features)\n\n# Extract features for classical ML comparison\nprint(\"Extracting classical features for comparison...\")\nX_train_classical = extract_features_classical(X_train[:100], config)\ny_train_classical = y_train[:100]\n\n# Train Random Forest for comparison\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_classical, y_train_classical)\n\n# Feature importance\nfeature_names = (\n    [f'MFCC_{i}_mean' for i in range(config.n_mfcc)] +\n    [f'MFCC_{i}_std' for i in range(config.n_mfcc)] +\n    [f'Chroma_{i}_mean' for i in range(12)] +\n    [f'Chroma_{i}_std' for i in range(12)] +\n    ['Spec_Centroid', 'Spec_Bandwidth', 'Rolloff', 'ZCR']\n)\n\nimportances = rf_model.feature_importances_\ntop_features_idx = np.argsort(importances)[-20:]\n\nfig = go.Figure(data=[\n    go.Bar(\n        x=importances[top_features_idx],\n        y=[feature_names[i] for i in top_features_idx],\n        orientation='h'\n    )\n])\nfig.update_layout(\n    title=\"Top 20 Most Important Features (Random Forest)\",\n    xaxis_title=\"Importance\",\n    yaxis_title=\"Feature\",\n    height=500\n)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.878388Z","iopub.status.idle":"2025-10-26T19:13:22.878639Z","shell.execute_reply.started":"2025-10-26T19:13:22.878523Z","shell.execute_reply":"2025-10-26T19:13:22.878536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 13: Model Interpretation (Attention Weights)\n# ============================================","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CELL 13: Model Interpretation (MODIFIED)\n# ============================================\n\n# Since we're using ensemble, let's analyze ensemble weights\nif config.model_type == 'ensemble':\n    # Check if model is wrapped in DataParallel\n    if isinstance(model, DataParallel):\n        weights = model.module.weights\n    else:\n        weights = model.weights\n    \n    weights_normalized = F.softmax(weights, dim=0)\n    \n    print(\"Ensemble Model Weights:\")\n    print(f\"  CNN Weight: {weights_normalized[0].item():.3f}\")\n    print(f\"  LSTM Weight: {weights_normalized[1].item():.3f}\")\n    print(f\"  Transformer Weight: {weights_normalized[2].item():.3f}\")\n    \n    # Visualize ensemble weights\n    import plotly.graph_objects as go\n    \n    fig = go.Figure(data=[\n        go.Bar(\n            x=['CNN', 'LSTM', 'Transformer'],\n            y=weights_normalized.detach().cpu().numpy(),\n            marker_color=['blue', 'green', 'red']\n        )\n    ])\n    fig.update_layout(\n        title=\"Ensemble Model Contribution Weights\",\n        yaxis_title=\"Weight\",\n        yaxis_range=[0, 1]\n    )\n    fig.show()\n\n# Analyze common misclassifications\nprint(\"\\n\" + \"=\"*60)\nprint(\"Most Common Misclassifications:\")\nprint(\"=\"*60)\n\n# Create confusion pairs\nconfusion_pairs = []\nfor true_idx in range(len(cm)):\n    for pred_idx in range(len(cm)):\n        if true_idx != pred_idx and cm[true_idx, pred_idx] > 10:\n            true_emotion = emotion_names[true_idx]\n            pred_emotion = emotion_names[pred_idx]\n            count = cm[true_idx, pred_idx]\n            confusion_pairs.append((true_emotion, pred_emotion, count))\n\n# Sort by frequency\nconfusion_pairs.sort(key=lambda x: x[2], reverse=True)\n\nfor true_em, pred_em, count in confusion_pairs[:10]:\n    print(f\"  {true_em:10s} misclassified as {pred_em:10s}: {count} times\")\n\n# Success rate by emotion\nprint(\"\\n\" + \"=\"*60)\nprint(\"Performance Summary by Emotion:\")\nprint(\"=\"*60)\n\nperformance = []\nfor i, emotion in enumerate(emotion_names[:config.n_classes]):\n    if i < len(per_class_acc):\n        total = cm[i].sum()\n        correct = cm[i, i]\n        performance.append({\n            'Emotion': emotion,\n            'Accuracy': per_class_acc[i],\n            'Correct': correct,\n            'Total': total,\n            'Errors': total - correct\n        })\n\n# Sort by accuracy\nperformance.sort(key=lambda x: x['Accuracy'], reverse=True)\n\nprint(f\"{'Rank':<5} {'Emotion':<10} {'Accuracy':<10} {'Correct/Total':<15}\")\nprint(\"-\" * 50)\nfor rank, perf in enumerate(performance, 1):\n    print(f\"{rank:<5} {perf['Emotion']:<10} {perf['Accuracy']*100:>6.1f}%    {perf['Correct']:>3}/{perf['Total']:<3}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.880322Z","iopub.status.idle":"2025-10-26T19:13:22.880581Z","shell.execute_reply.started":"2025-10-26T19:13:22.880428Z","shell.execute_reply":"2025-10-26T19:13:22.880437Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 14: Error Analysis\n# ============================================","metadata":{}},{"cell_type":"code","source":"def error_analysis(preds, labels, probs, emotion_names):\n    \"\"\"Analyze model errors\"\"\"\n    \n    # Find misclassified samples\n    errors = preds != labels\n    error_indices = np.where(errors)[0]\n    \n    if len(error_indices) > 0:\n        print(f\"Total errors: {len(error_indices)} / {len(labels)} ({100*len(error_indices)/len(labels):.1f}%)\")\n        \n        # Confusion pairs\n        confusion_pairs = {}\n        for idx in error_indices:\n            true_label = emotion_names[labels[idx]]\n            pred_label = emotion_names[preds[idx]]\n            pair = f\"{true_label} -> {pred_label}\"\n            confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n        \n        # Most common confusions\n        sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n        \n        print(\"\\nMost Common Confusions:\")\n        for pair, count in sorted_pairs[:10]:\n            print(f\"  {pair}: {count} times\")\n        \n        # Confidence analysis\n        correct_confidence = probs[~errors].max(axis=1).mean()\n        error_confidence = probs[errors].max(axis=1).mean()\n        \n        print(f\"\\nAverage Confidence:\")\n        print(f\"  Correct predictions: {correct_confidence:.3f}\")\n        print(f\"  Incorrect predictions: {error_confidence:.3f}\")\n        \n        # Plot confidence distribution\n        fig = go.Figure()\n        fig.add_trace(go.Histogram(\n            x=probs[~errors].max(axis=1),\n            name='Correct',\n            opacity=0.7,\n            nbinsx=30\n        ))\n        fig.add_trace(go.Histogram(\n            x=probs[errors].max(axis=1),\n            name='Incorrect',\n            opacity=0.7,\n            nbinsx=30\n        ))\n        fig.update_layout(\n            title=\"Confidence Distribution\",\n            xaxis_title=\"Confidence\",\n            yaxis_title=\"Count\",\n            barmode='overlay'\n        )\n        fig.show()\n\n# Perform error analysis\nerror_analysis(preds, labels, probs, emotion_names[:config.n_classes])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.881328Z","iopub.status.idle":"2025-10-26T19:13:22.881613Z","shell.execute_reply.started":"2025-10-26T19:13:22.881443Z","shell.execute_reply":"2025-10-26T19:13:22.881453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 15: Save Results and Model\n# ============================================","metadata":{}},{"cell_type":"code","source":"# Save results\nresults = {\n    'test_accuracy': accuracy,\n    'predictions': preds.tolist(),\n    'true_labels': labels.tolist(),\n    'probabilities': probs.tolist(),\n    'confusion_matrix': cm.tolist(),\n    'training_history': {\n        'train_losses': trainer.train_losses,\n        'val_losses': trainer.val_losses,\n        'train_accs': trainer.train_accs,\n        'val_accs': trainer.val_accs\n    }\n}\n\nwith open('/kaggle/working/results.json', 'w') as f:\n    json.dump(results, f)\n\nprint(\"Results saved to results.json\")\n\n# Save model for deployment\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'model_config': config,\n    'emotion_names': emotion_names[:config.n_classes],\n    'test_accuracy': accuracy\n}, '/kaggle/working/final_model.pth')\n\nprint(\"Model saved to final_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.882701Z","iopub.status.idle":"2025-10-26T19:13:22.882978Z","shell.execute_reply.started":"2025-10-26T19:13:22.882858Z","shell.execute_reply":"2025-10-26T19:13:22.882871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 16: Generate Project Report\n# ============================================","metadata":{}},{"cell_type":"code","source":"report = f\"\"\"\n# Speech Emotion Recognition Project Report\n\n## 1. Project Overview\n- **Objective**: Develop a deep learning system for emotion recognition from speech\n- **Model Type**: {config.model_type.upper()}\n- **Number of Classes**: {config.n_classes}\n- **Total Samples**: {len(file_paths)}\n- **Train/Val/Test Split**: {config.train_size}/{config.val_size}/{config.test_size}\n\n## 2. Model Architecture\n- **Parameters**: {sum(p.numel() for p in model.parameters()):,}\n- **Input Features**: Mel-spectrogram ({config.n_mels} bins)\n- **Batch Size**: {config.batch_size}\n- **Learning Rate**: {config.learning_rate}\n- **Epochs Trained**: {len(trainer.train_losses)}\n\n## 3. Performance Results\n- **Test Accuracy**: {accuracy:.4f}\n- **Best Validation Accuracy**: {checkpoint['val_acc']:.2f}%\n\n## 4. Per-Class Performance\n\"\"\"\n\nfor i, emotion in enumerate(emotion_names[:config.n_classes]):\n    if i < len(per_class_acc):\n        report += f\"- {emotion}: {per_class_acc[i]:.3f}\\n\"\n\nreport += \"\"\"\n## 5. Key Findings\n1. The model successfully learns to distinguish between different emotions\n2. Some emotion pairs show higher confusion rates (see error analysis)\n3. Ensemble models generally perform better than individual architectures\n\n## 6. Future Improvements\n1. Implement data augmentation techniques (pitch shift, time stretch)\n2. Try pre-trained models (Wav2Vec2, HuBERT)\n3. Collect more diverse training data\n4. Implement real-time emotion recognition\n\n## 7. Technologies Used\n- **Deep Learning**: PyTorch, TorchAudio\n- **Audio Processing**: Librosa\n- **Visualization**: Plotly\n- **Environment**: Kaggle GPU\n\"\"\"\n\nprint(report)\n\n# Save report\nwith open('/kaggle/working/project_report.md', 'w') as f:\n    f.write(report)\n\nprint(\"\\nProject completed successfully! ðŸŽ‰\")\nprint(\"Files saved:\")\nprint(\"- best_model.pth\")\nprint(\"- final_model.pth\")\nprint(\"- results.json\")\nprint(\"- project_report.md\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.884462Z","iopub.status.idle":"2025-10-26T19:13:22.884827Z","shell.execute_reply.started":"2025-10-26T19:13:22.884655Z","shell.execute_reply":"2025-10-26T19:13:22.884670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ============================================\n# CELL 17: Test Time Augmentation for Better Accuracy\n# ============================================","metadata":{}},{"cell_type":"code","source":"def evaluate_with_tta(model, test_loader, device, n_augmentations=5):\n    \"\"\"\n    Evaluate with Test Time Augmentation\n    This can improve accuracy by 0.5-2% without retraining!\n    \"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs_list = []\n    \n    print(f\"Applying Test Time Augmentation with {n_augmentations} augmentations per sample...\")\n    \n    with torch.no_grad():\n        for features, labels in tqdm(test_loader, desc=\"TTA Testing\"):\n            batch_probs = []\n            \n            # Original prediction\n            features_gpu = features.to(device)\n            with autocast():\n                outputs = model(features_gpu)\n            probs = F.softmax(outputs, dim=1)\n            batch_probs.append(probs.cpu())\n            \n            # Augmented predictions\n            for aug_idx in range(n_augmentations - 1):\n                # Apply different augmentations\n                aug_features = features.clone()\n                \n                if aug_idx == 0:\n                    # Add slight noise\n                    aug_features = aug_features + torch.randn_like(aug_features) * 0.003\n                elif aug_idx == 1:\n                    # Slight time shift\n                    shift_amount = torch.randint(-5, 5, (1,)).item()\n                    aug_features = torch.roll(aug_features, shifts=shift_amount, dims=-1)\n                elif aug_idx == 2:\n                    # Slight amplitude scaling\n                    scale = 1.0 + (torch.rand(1).item() - 0.5) * 0.1\n                    aug_features = aug_features * scale\n                else:\n                    # Random small perturbation\n                    aug_features = aug_features + torch.randn_like(aug_features) * 0.002\n                \n                aug_features = aug_features.to(device)\n                with autocast():\n                    aug_outputs = model(aug_features)\n                aug_probs = F.softmax(aug_outputs, dim=1)\n                batch_probs.append(aug_probs.cpu())\n            \n            # Average predictions from all augmentations\n            avg_probs = torch.stack(batch_probs).mean(dim=0)\n            _, predicted = avg_probs.max(1)\n            \n            all_preds.extend(predicted.numpy())\n            all_labels.extend(labels.numpy())\n            all_probs_list.append(avg_probs.numpy())\n    \n    return np.array(all_preds), np.array(all_labels), np.vstack(all_probs_list)\n\n# Apply TTA to your already trained model\nprint(\"=\"*60)\nprint(\"EVALUATING WITH TEST TIME AUGMENTATION\")\nprint(\"=\"*60)\n\n# Load best model if not already loaded\nif not 'model' in globals():\n    checkpoint = torch.load('/kaggle/working/best_model.pth', weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Loaded model with {checkpoint['val_acc']:.2f}% validation accuracy\")\n\n# Run TTA evaluation\ntta_preds, tta_labels, tta_probs = evaluate_with_tta(model, test_loader, device, n_augmentations=5)\n\n# Calculate improved metrics\ntta_accuracy = accuracy_score(tta_labels, tta_preds)\nprint(f\"\\nðŸŽ¯ Original Test Accuracy: 71.93%\")\nprint(f\"ðŸš€ TTA Test Accuracy: {tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)\")\nprint(f\"ðŸ“ˆ Improvement: +{(tta_accuracy - 0.7193)*100:.2f}%\")\n\n# Detailed classification report\nprint(\"\\n\" + \"=\"*60)\nprint(\"TTA Classification Report:\")\nprint(\"=\"*60)\nprint(classification_report(tta_labels, tta_preds, target_names=emotion_names[:config.n_classes], digits=3))\n\n# Confusion Matrix\ntta_cm = confusion_matrix(tta_labels, tta_preds)\nprint(\"\\n\" + \"=\"*60)\nprint(\"TTA Confusion Matrix:\")\nprint(\"=\"*60)\nprint(tta_cm)\n\n# Per-class accuracy\ntta_per_class_acc = tta_cm.diagonal() / tta_cm.sum(axis=1)\nprint(\"\\n\" + \"=\"*60)\nprint(\"TTA Per-Class Accuracy:\")\nprint(\"=\"*60)\nfor i, emotion in enumerate(emotion_names[:config.n_classes]):\n    if i < len(tta_per_class_acc):\n        improvement = (tta_per_class_acc[i] - per_class_acc[i]) * 100\n        print(f\"  {emotion:10s}: {tta_per_class_acc[i]:.3f} ({tta_per_class_acc[i]*100:.1f}%) [{'â†‘' if improvement > 0 else 'â†“'}{abs(improvement):.1f}%]\")\n\nprint(\"\\nâœ… TTA Evaluation Complete!\")\nprint(\"ðŸ’¡ TTA typically improves accuracy by 0.5-2% without any retraining!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T19:13:22.885743Z","iopub.status.idle":"2025-10-26T19:13:22.886050Z","shell.execute_reply.started":"2025-10-26T19:13:22.885894Z","shell.execute_reply":"2025-10-26T19:13:22.885907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Memory Management Tips for Kaggle\n\n```python\n# Add these between cells if you run out of memory\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Monitor GPU usage\n!nvidia-smi\n\n# Clear variables\ndel train_loader, val_loader  # After training\n```","metadata":{}}]}